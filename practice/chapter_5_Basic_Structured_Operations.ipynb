{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Basic Structured Operations\n",
    "- DataFrame: A distributed collection of data organized into named columns, similar to a table in a relational database or a data frame in R/Python.\n",
    "- Row: The individual records in a DataFrame.\n",
    "- Columns: Represents the computation expressions applicable to each record.\n",
    "- Schema: Provides a blueprint for the structure of the DataFrame, detailing column names and data types.\n",
    "- Partitioning: Crucial for performance, as it defines the data distribution across the Spark cluster nodes, optimizing resource utilization and parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/06 19:50:19 WARN Utils: Your hostname, Khanhs-MAC.local resolves to a loopback address: 127.0.0.1; using 192.168.0.102 instead (on interface en0)\n",
      "24/08/06 19:50:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/06 19:50:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "  .appName(\"PySpark Basic Structured Operations\")\\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schemas tie everything together, so they’re worth belaboring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas \n",
    "- Schema Definition: Defines column names and data types of a DataFrame.\n",
    "- Schema-on-Read: \n",
    "  - Automatically defines the schema when reading data.\n",
    "  - Suitable for ad hoc analysis.\n",
    "  - Works well with plain-text formats like CSV or JSON.\n",
    "  - Can be slow and may cause precision issues.\n",
    "- Explicit Schema Definition:\n",
    "  - Manually define the schema before reading data.\n",
    "  - Recommended for production ETL processes.\n",
    "  - Ensures accurate data type settings.\n",
    "  - Avoid precision issues.\n",
    "- Ad Hoc Analysis:\n",
    "  - Schema-on-read is generally sufficient for quick, exploratory analysis.\n",
    "- Precision Issues:\n",
    "  - Automated schema inference may not always be accurate.\n",
    "- ETL processes:\n",
    "  - Explicit schema definition is beneficial for performance and accuracy in structured workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Schema Composition:\n",
    "  - A schema is a StructType mae up of multiple StructFields.\n",
    "  - StructFields have:\n",
    "    - Name.\n",
    "    - Type.\n",
    "    - Boolean flag for null/missing values.\n",
    "    - Optional metadata for additional information.\n",
    "- Metadata:\n",
    "  - Used to store information about columns.\n",
    "  - Utilized by Spark's machine learning library.\n",
    "- Other StructType (Spark's Complex types):\n",
    "  - Schemas can include other StructTypes.\n",
    "- Runtime Type Checking:\n",
    "  - Spark throws an error if runtime data types do not match the schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[DEST_COUNTY_NAME: string, ORIGIN_COUNTY_NAME: string, count: bigint]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\": \"world\"})\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "  .load(\"../data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns and Expressions\n",
    "- Column Similarity:\n",
    "  - Columns in Spark are akin to columns in a spreadsheet, R, data frame, or pandas DataFrame.\n",
    "- Operations on Columns:\n",
    "  - You can select, manipulate, and remove columns from DataFrames.\n",
    "  - These operations are represented as expressions.\n",
    "- Logical Constructions:\n",
    "  - Columns are logical constructions representing a value computed per record using an expression\n",
    "- Dependency on DataFrame:\n",
    "  - A column's real value is tied to a row.\n",
    "  - To have a row, you need a DataFrame.\n",
    "  - Columns cannot be manipulated outside the context of a DataFrame.\n",
    "- Spark Transformations:\n",
    "  - Using Spark transformations within a DataFrame to modify column contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns\n",
    "- Two simplest ways: col and column functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'someColumnName'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "col(\"someColumnName\")\n",
    "column(\"someColumnName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Column Resolution:\n",
    "  - Columns may not exist in DataFrames util resolved against the catalog during the analyzer phase.\n",
    "- Scala Syntatic Sugar:\n",
    "  - $ and ' are shorthand ways to refer to columns.\n",
    "  - $ designates a string as a special string for expressions.\n",
    "  - ' is a symbol for referring to an identifier.\n",
    "  - Both provide no performance improvement but offer shorthand notation.\n",
    "- Explicit Column References:\n",
    "  - Use the col method on a specific DataFrame for explicit references.\n",
    "  - Useful for joining DataFrames with shared column names.\n",
    "  - This avoids the need for Spark to resolve the column during the analyzer phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expressions\n",
    "- An expression is a set of transformations on one or more value in a record within a DataFrame.\n",
    "- Functions as a computation on column names to produce a single value per record.\n",
    "- The single value can be a complex type like a Map or Array.\n",
    "- Creating Expressions:\n",
    "  - Simplest case: expr(\"someCol\") is equivalent to col(\"someCol\").\n",
    "  - expr function can parse transformations and column references from a string.\n",
    "- Columns as expressions:\n",
    "  - Columns are a subset of expressions functionality.\n",
    "  - Use col() for transformations on a column reference.\n",
    "  - Example: expr(\"someCol - 5\") is equivalent to col(\"someCol\") - 5.\n",
    "- Logical Plan:\n",
    "  - Columns and transformations compile to the same logical plan as parsed expressions.\n",
    "  - The logical plan is a directed acyclic graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SQL Equivalence:\n",
    "  - Expressions is expr can also be valid SQL code.\n",
    "  - SQL expressions and DataFrame code compile to the same logical tree, providing identical performance characteristics.\n",
    "- Accessing DataFrame's Columns:\n",
    "  - Use printSchema to see DataFrame's schema.\n",
    "  - Use the columns property to programmatically access all columns of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"../data/flight-data/json/2015-summary.json\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Records and Rows\n",
    "- Records and Rows in Spark:\n",
    "  - Each row in a DataFrame is single record, represents as an object of type Row.\n",
    "  - Spark manipulates Row objects using column expressions to produce usable values.\n",
    "  - Row objects internally represent arrays of bytes.\n",
    "- Byte Array Interface:\n",
    "  - The byte array interface of Row objects is not exposed to uses.\n",
    "  - Users interact with column expressions to manipulate Row objects.\n",
    "- Returning Rows:\n",
    "  - Commands that return individual rows to the driver will return one or more Row types when working with DataFrames.\n",
    "- Terminology Note:\n",
    "  - \"row\" and \"record\" are used interchangeably.\n",
    "  - A capitalized Row refers specifically to the Row object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Rows\n",
    "- Creating Rows:\n",
    "  - Row are manually instantiated using the Row object with values for each column.\n",
    "  - Only DataFrame have schemas; Rows do not.\n",
    "  - When creating a Row manually, values must be specified in the same order as the DataFrame schema they will be appended to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accessing Data in Rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[0]\n",
    "myRow[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Transformations\n",
    "- Core Parts of a DataFrame:\n",
    "  - Brief definition of core parts completed.\n",
    "- Manipulating DataFrames:\n",
    "  - Fundamental objective when working with DataFrames.\n",
    "- Core Operations:\n",
    " - Add rows or columns.\n",
    " - Remove rows or columns.\n",
    " - Transform a row into column or vice versa.\n",
    " - Change the order of rows based on values in columns.\n",
    "- Transformations can be simplified into operations that take one column, change it row by row, and then return the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "- DataFrame can be created from raw data sources.\n",
    "- Register DataFrames as temporary views for SQL queries and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\")\\\n",
    "  .load(\"../data/flight-data/json/2015-summary.json\")\n",
    "df.createOrReplaceGlobalTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating DataFrames on the fly:\n",
    "  - Convert a set of rows into a DataFrame.\n",
    "  - Define schema using StructType and StructField."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|            Hello|               NULL|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False)\n",
    "])\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Key Methods for DataFrames:\n",
    "  - *select* method for working with columns or expressions\n",
    "  - *selectExpr* method for working wit expressions in strings.\n",
    "  - Use functions from org.apache.spark.sql.functions for transformations not available as column methods.\n",
    "- Transformation Tools:\n",
    "  - With *select, selectExpr*, and *functions* from org.apache.spark.sql.functions, you can handle most DataFrame transformations challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select and selectExpr\n",
    "- *select* and *selectExpr*:\n",
    "  - Equivalent to SQL queries on a DataFrame.\n",
    "- USING *select* and *selectExpr*:\n",
    "  - Manipulate columns in DataFrames.\n",
    "  - Simplest way is to use the *select* method and pass column name as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Scala\n",
    "# df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "# in Python\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)\n",
    "# in SQL\n",
    "# SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)\n",
    "# in SQL\n",
    "#SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Referring to Columns:\n",
    "  - Columns can be referred to in multiple ways and used interchangeably.\n",
    "- Methods to Refer to Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, col, column\n",
    "\n",
    "df.select(\n",
    "  expr(\"DEST_COUNTRY_NAME\"),\n",
    "  col(\"DEST_COUNTRY_NAME\"),\n",
    "  column(\"DEST_COUNTRY_NAME\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Common Error:\n",
    " - Mixing Column objects and strings in the same select statement will result in a compiler error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, DEST_COUNTRY_NAME: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using expr for Flexibility:\n",
    "  - *expr* is the most flexible method to reference columns, as it can refer to a plain column or string manipulation of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "\n",
    "# -- in SQL\n",
    "# SELECT DEST_COUNTRY_NAME as destination FROM dfTable LIMIT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\"))\\\n",
    "  .alias(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has a shorthand for doing select and expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"DEST_COUNTRY_NAME AS newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Power of selectExpr:\n",
    "  - Allows building complex expressions to create new DataFrames.\n",
    "  - Can include any valid non-expression SQL statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "  \"*\",\n",
    "  \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With select expression, we can also specify aggregations over the entire DatFrame by taking advantage of the functions that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Spark Types (Literals)\n",
    "- Converting to Spark Types:\n",
    "  - Literals are used to pass explicit values into Spark that are just values, not new columns.\n",
    "  - Useful for constant values or for comparisons later.\n",
    "- Using Literals:\n",
    "  - Translate a literal value from a programming language to one that Spark understands.\n",
    "  - Literals are expressions and can be used similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns\n",
    "- Use the *withColumn* method to add a new column to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adding Columns with Expressions:\n",
    "  - Adding a column with a Boolean expression to check if the origin country is the same as the destination country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *withColumn* Function:\n",
    "  - Takes two arguments: the column name and the expression to create the value for each row.\n",
    "  - Can also be used to rename a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns\n",
    "- Renaming Columns:\n",
    "  - Use the *withColumnRenamed* method to rename a column.\n",
    "  - Renames the column with the given new name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destination', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"destination\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reversed Characters and Keywords\n",
    "- Use backticks (`) to handle reserved characters like spaces or dashes in column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't use escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "|    United States|            Romania|   15|              Romania|\n",
      "|    United States|            Croatia|    1|              Croatia|\n",
      "+-----------------+-------------------+-----+---------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName = df.withColumn(\n",
    "      \"This Long Column-Name\",\n",
    "      expr(\"ORIGIN_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Backticks for Reversed Characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'selectExpr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdfWithLongColName\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m(\n\u001b[1;32m      2\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`This Long Column-Name`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`This Long Column-Name` as `new col`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m dfWithLongColName\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdfTableLong\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# -- in SQL\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#   SELECT `This Long Column-Name`, `This Long Column-Name` as `new col`\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#   FROM dfTableLong LIMIT 2\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'selectExpr'"
     ]
    }
   ],
   "source": [
    "dfWithLongColName.selectExpr(\n",
    "      \"`This Long Column-Name`\",\n",
    "      \"`This Long Column-Name` as `new col`\")\\\n",
    "    .show(2)\n",
    "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")\n",
    "\n",
    "# -- in SQL\n",
    "#   SELECT `This Long Column-Name`, `This Long Column-Name` as `new col`\n",
    "#   FROM dfTableLong LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Sensitivity\n",
    "By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set spark.sql.caseSensitive true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Columns\n",
    "- We can do this by using *select*.\n",
    "- It also a dedicated method called *drop*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# We can drop multiple columns by passing in multiple columns as arguments\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdfWithLongColName\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mORIGIN_COUNTRY_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEST_COUNTRY_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "# We can drop multiple columns by passing in multiple columns as arguments\n",
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing a Column’s Type (cast)\n",
    "- We can do this by casting the column from one type to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint, count2: bigint]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn(\"count2\", col(\"count\").cast(\"long\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Rows\n",
    "- Create an expression that evaluates to true or false to filter rows.\n",
    "- Filter out rows the expression evaluates to false.\n",
    "- Methods for Filtering:\n",
    "  - *where*: Familiar to SQL.\n",
    "  - *filter*: Also valid and performs the same operation.\n",
    "  - Both methods are accepts the same arguments types and work with DataFrame.\n",
    "**Note**: When using the Dataset API from Scala or Java, filter accepts an arbitrary function to apply to each record in the Dataset.\n",
    "\n",
    "- Equivalent filters: The result of using *filter* and *where* are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"count\") < 2).show(2)\n",
    "df.where(\"count < 2\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiple filters:\n",
    "  - You. can chain multiple filters sequentially.\n",
    "  - Spark performs all filtering operations at the same time regardless of the filter ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\").show(2)\n",
    "# -- in SQL\n",
    "#   SELECT * FROM dfTable WHERE count < 2 AND ORIGIN_COUNTRY_NAME != \"Croatia\"\n",
    "#   LIMIT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Unique Rows\n",
    "- Extract the unique or distinct in a DataFrame using the *distinct* method.\n",
    "- Deduplicate rows based on one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Samples\n",
    "- Use the *sample* on a DataFrame to extract random records.\n",
    "- Specify the fraction of rows to extract and whether to sample with or without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating and Appending Rows (Union)\n",
    "- DataFrame are Immutable:\n",
    "  - Cannot append directly to DataFrame as that would change them.\n",
    "  - To append, use the union method to concatenate the original DataFrame.\n",
    "- Union Requirement:\n",
    "  - Both DataFrames must have the same schema and number of columns.\n",
    "  - The union operation will fail if schemas or column counts do not match.\n",
    "- **Warning:**\n",
    "  - Unions are performed based on column locations, not schema.\n",
    "  - Columns may not automatically line up as expected if their positions differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df.schema\n",
    "newRows = [\n",
    "    Row(\"New Country\", \"Other Country\", 5),\n",
    "    Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|          Gibraltar|    1|\n",
      "|    United States|             Cyprus|    1|\n",
      "|    United States|            Estonia|    1|\n",
      "|    United States|          Lithuania|    1|\n",
      "|    United States|           Bulgaria|    1|\n",
      "|    United States|            Georgia|    1|\n",
      "|    United States|            Bahrain|    1|\n",
      "|    United States|   Papua New Guinea|    1|\n",
      "|    United States|         Montenegro|    1|\n",
      "|    United States|            Namibia|    1|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(newDF)\\\n",
    "    .where(\"count = 1\")\\\n",
    "    .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Rows \n",
    "- Sort values in a DataFrame to place the largest or smallest values at the top.\n",
    "- Two equivalent operations: sort and orderBy.\n",
    "- Both methods accept column expressions, strings, and multiple columns.\n",
    "- Default sort order is ascending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"count\").show(5)\n",
    "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Moldova|      United States|    1|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "df.orderBy(expr(\"count desc\")).show(2)\n",
    "df.orderBy(expr(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format(\"json\").load(\"../data/flight-data/json/*-summary.json\")\\\n",
    "  .sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repartition and Coalesce\n",
    "- Repartition and Coalesce:\n",
    "\t- Important for optimizing data layout across the cluster.\n",
    "\t- Controls the physical layout of data including the partitioning scheme and the number of partitions.\n",
    "- Repartition:\n",
    "\t- Incurs a full shuffle of data, regardless of necessity.\n",
    "\t- Recommended when the future number of partitions is greater than the current number or when partitioning by specific columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|           Greece|      United States|   30|\n",
      "|    United States|            Bermuda|  193|\n",
      "|    United States|           Portugal|  134|\n",
      "|    United States|Trinidad and Tobago|  217|\n",
      "|          Romania|      United States|   14|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "df.repartition(5).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartition by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|         Anguilla|      United States|   41|\n",
      "|           Russia|      United States|  176|\n",
      "|         Paraguay|      United States|   60|\n",
      "|          Senegal|      United States|   40|\n",
      "|           Sweden|      United States|  118|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|             Moldova|      United States|    1|\n",
      "|             Bolivia|      United States|   30|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "|            Pakistan|      United States|   12|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Rows to the Driver\n",
    "- Collecting Rows to the Driver:\n",
    "  - Spark maintains the state of the cluster in the driver.\n",
    "  - Collect data to the driver for local manipulation.\n",
    "- Methods for Collecting Data:\n",
    "  - collect: Retrieves all data from the entire DataFrame.\n",
    "  - take(N): Selects the first N rows.\n",
    "  - show(N): Prints out a specified number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x1196f9150>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Warning on Collecting Data to the Driver:\n",
    "  - Collecting data to the driver can be very expensive.\n",
    "  - Large datasets may crash the driver when using collect.\n",
    "  - Using toLocalIterator with large partitions can crash the driver node and lose the application state.\n",
    "  - This approach is also expensive as it operates on a one-by-one basis, instead of running computations in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
